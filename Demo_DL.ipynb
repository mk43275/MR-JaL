{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# import libraries\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "from transformers import Wav2Vec2Model, Wav2Vec2FeatureExtractor\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import librosa\n",
        "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
        "!pip install datasets\n",
        "from datasets import load_dataset\n",
        "!pip install gradio\n",
        "import matplotlib.pyplot as plt\n",
        "import gradio as gr"
      ],
      "metadata": {
        "id": "okXbqnAzAZyG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Mojo of reproducibility\n",
        "def set_seed(seed):\n",
        "  #PyTorch\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed_all(seed)\n",
        "  #Numpy\n",
        "  np.random.seed(seed)\n",
        "  #Python_random\n",
        "  random.seed(seed)\n",
        "  #CuDNN (when using CUDA)\n",
        "  if torch.cuda.is_available():\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "# Define the audio to emotion model using Wav2Vec2\n",
        "class wav2Vec_transferModel(nn.Module):\n",
        "    def __init__(self, pretrained_model_name=\"r-f/wav2vec-english-speech-emotion-recognition\"):\n",
        "        super().__init__()\n",
        "\n",
        "        # Load pre-trained model\n",
        "        self.model = Wav2Vec2Model.from_pretrained(pretrained_model_name)\n",
        "\n",
        "        # Freeze the pre-trained Wav2Vec2 model\n",
        "        # Only the last classifying layers will be trained\n",
        "        for param in self.model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # Define dropout layer to avoid overfitting in the classifier\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "\n",
        "        # Attaching a new classifying layers for emotion label and emotion strength\n",
        "        self.classifier_emo = nn.Sequential(\n",
        "            nn.Linear(self.model.config.hidden_size, 128),\n",
        "            # nn.BatchNorm1d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 128),\n",
        "            # nn.BatchNorm1d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64,32),\n",
        "            # nn.BatchNorm1d(32),\n",
        "            nn.ReLU(),\n",
        "            self.dropout,\n",
        "            nn.Linear(32, 6)\n",
        "        )\n",
        "\n",
        "        self.classifier_strength = nn.Sequential(\n",
        "            nn.Linear(self.model.config.hidden_size, 128),\n",
        "            # nn.BatchNorm1d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 64),\n",
        "            # nn.BatchNorm1d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 32),\n",
        "            # nn.BatchNorm1d(32),\n",
        "            nn.ReLU(),\n",
        "            self.dropout,\n",
        "            nn.Linear(32, 3)\n",
        "        )\n",
        "\n",
        "        # Initialize weights by Xavier initialization\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        for layer in self.modules():\n",
        "            if isinstance(layer, nn.Linear):\n",
        "                nn.init.xavier_uniform_(layer.weight)\n",
        "                if layer.bias is not None:\n",
        "                    nn.init.zeros_(layer.bias)\n",
        "\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # Forward pass through the Wav2Vec2 model to extract the hidden states\n",
        "        # Disable gradient calculation for the Wav2Vec2 model part\n",
        "        # Input is a feature extracted by Wav2Vec2FeatureExtractor beforehand\n",
        "        input_values = inputs['input_values'].squeeze(1)\n",
        "        attention_mask = inputs['attention_mask'].squeeze(1)  # if needed\n",
        "\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(input_values=input_values, attention_mask=attention_mask)\n",
        "            # Extract the last hidden state.\n",
        "            # The last hidden state is a tensor of shape (batch_size, sequence_length, hidden_size)\n",
        "            # Use mean pooling over the sequenth length to get a fixed-size representation\n",
        "            last_hidden_state_pooled = outputs.last_hidden_state.mean(1)\n",
        "\n",
        "        # Pass the hidden states through the classifier\n",
        "        logits_emo = self.classifier_emo(last_hidden_state_pooled)\n",
        "        logits_strength = self.classifier_strength(last_hidden_state_pooled)\n",
        "\n",
        "        # Return the logits for both tasks\n",
        "        return logits_emo, logits_strength\n",
        "\n",
        "# emotion detection model\n",
        "class emotionDetectionSystem():\n",
        "    def __init__(self, useAudio = True, useText = True):\n",
        "        self.useAudio = useAudio\n",
        "        self.useText = useText\n",
        "        self.device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "        # If using audio, Load the Wav2Vec2 model for audio emotion recognition\n",
        "        if self.useAudio:\n",
        "            self.wav2vec_modelName = \"r-f/wav2vec-english-speech-emotion-recognition\"\n",
        "            self.wav2vec_model = wav2Vec_transferModel()\n",
        "            self.wav2vec_model.load_state_dict(torch.load('bestWav2Vec_balanced.pth'))\n",
        "            self.wav2vec_model.eval()\n",
        "            self.wav2vec_model.to(self.device)\n",
        "            self.wav2vec_model.requires_grad = False\n",
        "\n",
        "        # If using text, Load the BERT model for text emotion recognition\n",
        "        if self.useText:\n",
        "            self.textClassifier_model_name = \"michellejieli/emotion_text_classifier\"\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(self.textClassifier_model_name)\n",
        "            self.textModel = AutoModelForSequenceClassification.from_pretrained(self.textClassifier_model_name)\n",
        "            self.textModel.to(self.device)\n",
        "            self.textModel.eval()\n",
        "            self.textModel.requires_grad = False\n",
        "\n",
        "            # Also load the dictation model\n",
        "            self.torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
        "            self.transcriptModel_id = \"openai/whisper-medium\"\n",
        "\n",
        "            self.transcriptModel = AutoModelForSpeechSeq2Seq.from_pretrained(self.transcriptModel_id,\n",
        "                                                                             torch_dtype=self.torch_dtype,\n",
        "                                                                             low_cpu_mem_usage=True,\n",
        "                                                                             use_safetensors=True)\n",
        "            self.transcriptModel.to(self.device)\n",
        "\n",
        "            self.transcriptProcessor = AutoProcessor.from_pretrained(self.transcriptModel_id)\n",
        "\n",
        "            self.transcriptPipe = pipeline(\n",
        "                \"automatic-speech-recognition\",\n",
        "                model=self.transcriptModel,\n",
        "                tokenizer=self.transcriptProcessor.tokenizer,\n",
        "                feature_extractor=self.transcriptProcessor.feature_extractor,\n",
        "                torch_dtype=self.torch_dtype,\n",
        "                device=self.device,\n",
        "            )\n",
        "\n",
        "\n",
        "    def predict(self, input = None):\n",
        "        # audio, sr = librosa.load(audio_path, sr=16000)\n",
        "        sr, audio = input\n",
        "\n",
        "        if self.useAudio:\n",
        "            audio = torch.tensor(audio)\n",
        "            feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(self.wav2vec_modelName)\n",
        "            audioInput = feature_extractor(audio, sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
        "            audioInput = audioInput.to(self.device)\n",
        "            with torch.no_grad():\n",
        "                audio_logits_emo, audio_logits_strength = self.wav2vec_model(audioInput)\n",
        "                audio_probs_emo = F.softmax(audio_logits_emo, dim=1)\n",
        "                audio_probs_strength = F.softmax(audio_logits_strength, dim=1)\n",
        "\n",
        "        if self.useText:\n",
        "            transcript = self.transcriptPipe(np.array(audio))\n",
        "            transcript = transcript[\"text\"]\n",
        "            inputs = self.tokenizer(transcript, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "            inputs = inputs.to(self.device)\n",
        "            with torch.no_grad():\n",
        "                text_logits_emo = self.textModel(**inputs).logits\n",
        "                text_probs_emo_raw = F.softmax(text_logits_emo, dim=1)[0]\n",
        "                # Re-order the probabilities to match the output of the audio model\n",
        "                # Please check if the order is correct!!!\n",
        "                text_emo_probs_reordered = torch.tensor([text_probs_emo_raw[4],\n",
        "                                                         text_probs_emo_raw[0],\n",
        "                                                         text_probs_emo_raw[1],\n",
        "                                                         #Adding the probability of \"Surprise\" to \"Fear\"\n",
        "                                                         text_probs_emo_raw[2] + text_probs_emo_raw[6],\n",
        "                                                         text_probs_emo_raw[5],\n",
        "                                                         text_probs_emo_raw[3]]\n",
        "                                                        )\n",
        "                text_emo_probs_reordered = text_emo_probs_reordered.to(self.device)\n",
        "\n",
        "\n",
        "        if self.useAudio and self.useText:\n",
        "            # Average the probabilities from both models\n",
        "            emo_probs = (audio_probs_emo + text_emo_probs_reordered) / 2\n",
        "            strength_probs = audio_probs_strength\n",
        "\n",
        "        elif self.useAudio:\n",
        "            emo_probs = audio_probs_emo\n",
        "            strength_probs = audio_probs_strength\n",
        "        elif self.useText:\n",
        "            emo_probs = text_emo_probs_reordered\n",
        "            strength_probs = None\n",
        "        else:\n",
        "            raise ValueError(\"At least one of useAudio or useText must be True.\")\n",
        "\n",
        "        emoList = [\"Neutral\", \"Anger\", \"Disgust\", \"Fear\", \"Sadness\", \"Happiness\"]\n",
        "        strengthList = [\"Weak\", \"Medium\", \"Strong\"]\n",
        "\n",
        "        # Get the predicted emotion and strength\n",
        "        emo_pred = emoList[torch.argmax(emo_probs).item()]\n",
        "        strength_pred = strengthList[torch.argmax(strength_probs).item()] if strength_probs is not None else None\n",
        "\n",
        "        print(f\"Predicted Emotion: {emo_pred}\")\n",
        "        if strength_pred is not None:\n",
        "            print(f\"Predicted Strength: {strength_pred}\")\n",
        "\n",
        "        pred_dict = {}\n",
        "        for i,j in zip(emoList, emo_probs[0]):\n",
        "          pred_dict[i] = float(j)\n",
        "\n",
        "        return pred_dict, strength_pred\n",
        "        # return emo_pred, strength_pred\n",
        "\n",
        "emoclassifier = emotionDetectionSystem()\n",
        "\n",
        "def emotionClassify(p):\n",
        "  return emoclassifier.predict(p)\n",
        "\n",
        "input_audio = gr.Audio(label=\"Upload Audio for Emotion Classification\")\n",
        "output_label = gr.Label(label = 'Predicted Emotions Probablities')\n",
        "\n",
        "demo = gr.Interface(\n",
        "    fn=emotionClassify,\n",
        "    inputs=input_audio,\n",
        "    outputs= [output_label, gr.Textbox(label=\"Prediction Strength\")]\n",
        ")"
      ],
      "metadata": {
        "id": "znwfuT2vpFPT"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "demo.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 827
        },
        "id": "Q7mlqxXPpIOP",
        "outputId": "1c9260d6-7be8-4670-b7fe-9d14743879d4"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://724b5fabbe083c27e7.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://724b5fabbe083c27e7.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/models/whisper/generation_whisper.py:573: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Emotion: Neutral\n",
            "Predicted Strength: Strong\n",
            "Predicted Emotion: Neutral\n",
            "Predicted Strength: Strong\n",
            "Predicted Emotion: Neutral\n",
            "Predicted Strength: Strong\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://724b5fabbe083c27e7.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    }
  ]
}