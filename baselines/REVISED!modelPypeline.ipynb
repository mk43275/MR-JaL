{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Revised code for base pipeline"
      ],
      "metadata": {
        "id": "p1pJl_Syv9k2"
      },
      "id": "p1pJl_Syv9k2"
    },
    {
      "cell_type": "code",
      "source": [
        "# Import essential libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "import numpy as np\n",
        "import librosa\n",
        "\n",
        "# HuggingFace Transformers modules\n",
        "from transformers import (\n",
        "    Wav2Vec2Model, Wav2Vec2FeatureExtractor,\n",
        "    AutoTokenizer, AutoModelForSequenceClassification,\n",
        "    AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
        ")\n",
        "\n",
        "# Set seeds for reproducibility\n",
        "def set_seed(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed(42)"
      ],
      "metadata": {
        "id": "sm9nJVL7tY5q"
      },
      "id": "sm9nJVL7tY5q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wav2Vec"
      ],
      "metadata": {
        "id": "5auun-Y9t4Rf"
      },
      "id": "5auun-Y9t4Rf"
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Wav2Vec2-based model for emotion + strength prediction\n",
        "class wav2Vec_transferModel(nn.Module):\n",
        "    def __init__(self, pretrained_model_name=\"r-f/wav2vec-english-speech-emotion-recognition\"):\n",
        "        super().__init__()\n",
        "\n",
        "        # Load pre-trained Wav2Vec2 model (frozen for feature extraction)\n",
        "        self.model = Wav2Vec2Model.from_pretrained(pretrained_model_name)\n",
        "        for param in self.model.parameters():\n",
        "            param.requires_grad = False  # Freeze all layers\n",
        "\n",
        "        # Dropout to reduce overfitting\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "\n",
        "        # Emotion classification head\n",
        "        self.classifier_emo = nn.Sequential(\n",
        "            nn.Linear(self.model.config.hidden_size, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 32),\n",
        "            nn.ReLU(),\n",
        "            self.dropout,\n",
        "            nn.Linear(32, 6)  # 6 emotion classes\n",
        "        )\n",
        "\n",
        "        # Strength classification head\n",
        "        self.classifier_strength = nn.Sequential(\n",
        "            nn.Linear(self.model.config.hidden_size, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 32),\n",
        "            nn.ReLU(),\n",
        "            self.dropout,\n",
        "            nn.Linear(32, 3)  # 3 strength levels\n",
        "        )\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    # Xavier initialization for classifier weights\n",
        "    def init_weights(self):\n",
        "        for layer in self.modules():\n",
        "            if isinstance(layer, nn.Linear):\n",
        "                nn.init.xavier_uniform_(layer.weight)\n",
        "                if layer.bias is not None:\n",
        "                    nn.init.zeros_(layer.bias)\n",
        "\n",
        "    # Forward pass: returns logits for emotion and strength\n",
        "    def forward(self, inputs):\n",
        "        input_values = inputs['input_values'].squeeze(1)\n",
        "        attention_mask = inputs['attention_mask'].squeeze(1)\n",
        "\n",
        "        with torch.no_grad():  # Wav2Vec2 is frozen\n",
        "            outputs = self.model(input_values=input_values, attention_mask=attention_mask)\n",
        "            # Use mean pooling over time dimension\n",
        "            last_hidden_state_pooled = outputs.last_hidden_state.mean(1)\n",
        "\n",
        "        # Pass through both heads\n",
        "        logits_emo = self.classifier_emo(last_hidden_state_pooled)\n",
        "        logits_strength = self.classifier_strength(last_hidden_state_pooled)\n",
        "\n",
        "        return logits_emo, logits_strength\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wqwNingjxO78"
      },
      "id": "wqwNingjxO78",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "audio + text"
      ],
      "metadata": {
        "id": "ibTa-Np5umEA"
      },
      "id": "ibTa-Np5umEA"
    },
    {
      "cell_type": "code",
      "source": [
        "# Main class for full emotion detection pipeline (audio + text)\n",
        "class emotionDetectionSystem():\n",
        "    def __init__(self, useAudio=True, useText=True):\n",
        "        self.useAudio = useAudio\n",
        "        self.useText = useText\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "        # Load Wav2Vec2 model for audio emotion recognition\n",
        "        if self.useAudio:\n",
        "            self.wav2vec_modelName = \"r-f/wav2vec-english-speech-emotion-recognition\"\n",
        "            self.wav2vec_model = wav2Vec_transferModel()\n",
        "            self.wav2vec_model.load_state_dict(torch.load('bestWav2Vec.pth', map_location=self.device))\n",
        "            self.wav2vec_model.eval()\n",
        "            self.wav2vec_model.to(self.device)\n",
        "\n",
        "        # Load BERT-based text emotion classifier and Whisper transcription model\n",
        "        if self.useText:\n",
        "            self.textClassifier_model_name = \"michellejieli/emotion_text_classifier\"\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(self.textClassifier_model_name)\n",
        "            self.textModel = AutoModelForSequenceClassification.from_pretrained(self.textClassifier_model_name)\n",
        "            self.textModel.to(self.device)\n",
        "            self.textModel.eval()\n",
        "\n",
        "            # Load Whisper for audio-to-text transcription\n",
        "            self.transcriptModel_id = \"openai/whisper-large-v3-turbo\"\n",
        "            self.torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
        "\n",
        "            self.transcriptModel = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
        "                self.transcriptModel_id, torch_dtype=self.torch_dtype,\n",
        "                low_cpu_mem_usage=True, use_safetensors=True\n",
        "            ).to(self.device)\n",
        "\n",
        "            self.transcriptProcessor = AutoProcessor.from_pretrained(self.transcriptModel_id)\n",
        "\n",
        "            # Pipeline for easy speech-to-text conversion\n",
        "            self.transcriptPipe = pipeline(\n",
        "                \"automatic-speech-recognition\",\n",
        "                model=self.transcriptModel,\n",
        "                tokenizer=self.transcriptProcessor.tokenizer,\n",
        "                feature_extractor=self.transcriptProcessor.feature_extractor,\n",
        "                torch_dtype=self.torch_dtype,\n",
        "                device=0 if self.device == \"cuda\" else -1,\n",
        "            )\n",
        "\n",
        "    # Prediction function that runs audio + optional transcription-based emotion detection\n",
        "    def predict(self, audio_path=None):\n",
        "        # Load and preprocess audio to 16kHz\n",
        "        audio, sr = librosa.load(audio_path, sr=16000)\n",
        "\n",
        "        if self.useAudio:\n",
        "            # Extract features using Wav2Vec2's feature extractor\n",
        "            audio_tensor = torch.tensor(audio).unsqueeze(0)\n",
        "            feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(self.wav2vec_modelName)\n",
        "            audioInput = feature_extractor(audio_tensor, sampling_rate=16000, return_tensors=\"pt\", padding=True).to(self.device)\n",
        "\n",
        "            # Run emotion and strength prediction\n",
        "            with torch.no_grad():\n",
        "                audio_logits_emo, audio_logits_strength = self.wav2vec_model(audioInput)\n",
        "                audio_probs_emo = F.softmax(audio_logits_emo, dim=1)[0]\n",
        "                audio_probs_strength = F.softmax(audio_logits_strength, dim=1)[0]\n",
        "\n",
        "        if self.useText:\n",
        "            # Transcribe audio using Whisper\n",
        "            transcript = self.transcriptPipe(audio_path)[\"text\"]\n",
        "            inputs = self.tokenizer(transcript, return_tensors=\"pt\", padding=True, truncation=True).to(self.device)\n",
        "\n",
        "            # Run text-based emotion prediction\n",
        "            with torch.no_grad():\n",
        "                text_logits_emo = self.textModel(**inputs).logits\n",
        "                text_probs_emo_raw = F.softmax(text_logits_emo, dim=1)[0]\n",
        "\n",
        "                # Align label order between audio and text models\n",
        "                text_emo_probs_reordered = torch.tensor([\n",
        "                    text_probs_emo_raw[4],  # Neutral\n",
        "                    text_probs_emo_raw[0],  # Anger\n",
        "                    text_probs_emo_raw[1],  # Disgust\n",
        "                    text_probs_emo_raw[2] + text_probs_emo_raw[6],  # Fear + Surprise\n",
        "                    text_probs_emo_raw[5],  # Sadness\n",
        "                    text_probs_emo_raw[3],  # Happiness\n",
        "                ], device=self.device)\n",
        "\n",
        "        # Combine or choose available predictions\n",
        "        if self.useAudio and self.useText:\n",
        "            emo_probs = (audio_probs_emo + text_emo_probs_reordered) / 2\n",
        "            strength_probs = audio_probs_strength\n",
        "        elif self.useAudio:\n",
        "            emo_probs = audio_probs_emo\n",
        "            strength_probs = audio_probs_strength\n",
        "        elif self.useText:\n",
        "            emo_probs = text_emo_probs_reordered\n",
        "            strength_probs = None\n",
        "        else:\n",
        "            raise ValueError(\"At least one of useAudio or useText must be True.\")\n",
        "\n",
        "        # Class labels\n",
        "        emoList = [\"Neutral\", \"Anger\", \"Disgust\", \"Fear\", \"Sadness\", \"Happiness\"]\n",
        "        strengthList = [\"Weak\", \"Medium\", \"Strong\"]\n",
        "\n",
        "        # Final predicted class labels\n",
        "        emo_pred = emoList[torch.argmax(emo_probs).item()]\n",
        "        strength_pred = strengthList[torch.argmax(strength_probs).item()] if strength_probs is not None else None\n",
        "\n",
        "        # Print predictions\n",
        "        print(f\"\\nPredicted Emotion: {emo_pred}\")\n",
        "        if strength_pred:\n",
        "            print(f\"Predicted Strength: {strength_pred}\")\n",
        "\n",
        "        print(\"\\nEmotion Probabilities:\")\n",
        "        for i, prob in enumerate(emo_probs):\n",
        "            print(f\"{emoList[i]}: {prob.item():.4f}\")\n",
        "\n",
        "        if strength_probs is not None:\n",
        "            print(\"\\nStrength Probabilities:\")\n",
        "            for i, prob in enumerate(strength_probs):\n",
        "                print(f\"{strengthList[i]}: {prob.item():.4f}\")\n",
        "\n",
        "        return emo_pred, strength_pred\n"
      ],
      "metadata": {
        "id": "0rOyy99OunVc"
      },
      "id": "0rOyy99OunVc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Previous Code\n",
        "what I based the changes off of"
      ],
      "metadata": {
        "id": "JgYqpetnul9v"
      },
      "id": "JgYqpetnul9v"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3b0077f",
      "metadata": {
        "id": "d3b0077f"
      },
      "outputs": [],
      "source": [
        "#Mojo of reproducibility\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "def set_seed(seed):\n",
        "  #PyTorch\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed_all(seed)\n",
        "  #Numpy\n",
        "  np.random.seed(seed)\n",
        "  #Python_random\n",
        "  random.seed(seed)\n",
        "  #CuDNN (when using CUDA)\n",
        "  if torch.cuda.is_available():\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2983632",
      "metadata": {
        "id": "f2983632"
      },
      "outputs": [],
      "source": [
        "# Define the audio to emotion model using Wav2Vec2\n",
        "class wav2Vec_transferModel(nn.Module):\n",
        "    def __init__(self, pretrained_model_name=\"r-f/wav2vec-english-speech-emotion-recognition\"):\n",
        "        super().__init__()\n",
        "\n",
        "        # Load pre-trained model\n",
        "        self.model = Wav2Vec2Model.from_pretrained(pretrained_model_name)\n",
        "\n",
        "        # Freeze the pre-trained Wav2Vec2 model\n",
        "        # Only the last classifying layers will be trained\n",
        "        for param in self.model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # Define dropout layer to avoid overfitting in the classifier\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "\n",
        "        # Attaching a new classifying layers for emotion label and emotion strength\n",
        "        self.classifier_emo = nn.Sequential(\n",
        "            nn.Linear(self.model.config.hidden_size, 128),\n",
        "            # nn.BatchNorm1d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 128),\n",
        "            # nn.BatchNorm1d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64,32),\n",
        "            # nn.BatchNorm1d(32),\n",
        "            nn.ReLU(),\n",
        "            self.dropout,\n",
        "            nn.Linear(32, 6)\n",
        "        )\n",
        "\n",
        "        self.classifier_strength = nn.Sequential(\n",
        "            nn.Linear(self.model.config.hidden_size, 128),\n",
        "            # nn.BatchNorm1d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 64),\n",
        "            # nn.BatchNorm1d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 32),\n",
        "            # nn.BatchNorm1d(32),\n",
        "            nn.ReLU(),\n",
        "            self.dropout,\n",
        "            nn.Linear(32, 3)\n",
        "        )\n",
        "\n",
        "        # Initialize weights by Xavier initialization\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        for layer in self.modules():\n",
        "            if isinstance(layer, nn.Linear):\n",
        "                nn.init.xavier_uniform_(layer.weight)\n",
        "                if layer.bias is not None:\n",
        "                    nn.init.zeros_(layer.bias)\n",
        "\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # Forward pass through the Wav2Vec2 model to extract the hidden states\n",
        "        # Disable gradient calculation for the Wav2Vec2 model part\n",
        "        # Input is a feature extracted by Wav2Vec2FeatureExtractor beforehand\n",
        "        input_values = inputs['input_values'].squeeze(1)\n",
        "        attention_mask = inputs['attention_mask'].squeeze(1)  # if needed\n",
        "\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(input_values=input_values, attention_mask=attention_mask)\n",
        "            # Extract the last hidden state.\n",
        "            # The last hidden state is a tensor of shape (batch_size, sequence_length, hidden_size)\n",
        "            # Use mean pooling over the sequenth length to get a fixed-size representation\n",
        "            last_hidden_state_pooled = outputs.last_hidden_state.mean(1)\n",
        "\n",
        "        # Pass the hidden states through the classifier\n",
        "        logits_emo = self.classifier_emo(last_hidden_state_pooled)\n",
        "        logits_strength = self.classifier_strength(last_hidden_state_pooled)\n",
        "\n",
        "        # Return the logits for both tasks\n",
        "        return logits_emo, logits_strength"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69f2271e",
      "metadata": {
        "id": "69f2271e"
      },
      "outputs": [],
      "source": [
        "from transformers import Wav2Vec2Model, Wav2Vec2FeatureExtractor\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import librosa\n",
        "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
        "from datasets import load_dataset\n",
        "\n",
        "\n",
        "class emotionDetectionSystem():\n",
        "    def __init__(self, useAudio = True, useText = True):\n",
        "        self.useAudio = useAudio\n",
        "        self.useText = useText\n",
        "        self.device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "        # If using audio, Load the Wav2Vec2 model for audio emotion recognition\n",
        "        if self.useAudio:\n",
        "            self.wav2vec_modelName = \"r-f/wav2vec-english-speech-emotion-recognition\"\n",
        "            self.wav2vec_model = wav2Vec_transferModel()\n",
        "            self.wav2vec_model.load_state_dict(torch.load('bestWav2Vec.pth'))\n",
        "            self.wav2vec_model.eval()\n",
        "            self.wav2vec_model.to(self.device)\n",
        "            self.wav2vec_model.requires_grad = False\n",
        "\n",
        "        # If using text, Load the BERT model for text emotion recognition\n",
        "        if self.useText:\n",
        "            self.textClassifier_model_name = \"michellejieli/emotion_text_classifier\"\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(self.textClassifier_model_name)\n",
        "            self.textModel = AutoModelForSequenceClassification.from_pretrained(self.textClassifier_model_name)\n",
        "            self.textModel.to(self.device)\n",
        "            self.textModel.eval()\n",
        "            self.textModel.requires_grad = False\n",
        "\n",
        "            # Also load the dictation model\n",
        "            self.torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
        "            self.transcriptModel_id = \"openai/whisper-large-v3-turbo\"\n",
        "\n",
        "            self.transcriptModel = AutoModelForSpeechSeq2Seq.from_pretrained(transcriptModel_id,\n",
        "                                                                             torch_dtype=torch_dtype,\n",
        "                                                                             low_cpu_mem_usage=True,\n",
        "                                                                             use_safetensors=True)\n",
        "            self.transcriptModel.to(device)\n",
        "\n",
        "            self.transcriptProcessor = AutoProcessor.from_pretrained(transcriptModel_id)\n",
        "\n",
        "            self.transcriptPipe = pipeline(\n",
        "                \"automatic-speech-recognition\",\n",
        "                model=self.transcriptModel,\n",
        "                tokenizer=self.transcriptProcessor.tokenizer,\n",
        "                feature_extractor=self.transcriptProcessor.feature_extractor,\n",
        "                torch_dtype=self.torch_dtype,\n",
        "                device=self.device,\n",
        "            )\n",
        "\n",
        "\n",
        "    def predict(self, audio_path = None):\n",
        "        audio, sr = librosa.load(audio_path, sr=16000)\n",
        "\n",
        "        if self.useAudio:\n",
        "            audio = torch.tensor(audio)\n",
        "            feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(self.wav2vec_modelName)\n",
        "            audioInput = feature_extractor(audio, sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
        "            audioInput = audioInput.to(device)\n",
        "            with torch.no_grad():\n",
        "                audio_logits_emo, audio_logits_strength = self.wav2vec_model(audioInput)\n",
        "                audio_probs_emo = F.softmax(logits_emo, dim=1)\n",
        "                audio_probs_strength = F.softmax(logits_strength, dim=1)\n",
        "\n",
        "        if self.useText:\n",
        "            transcript = self.transcriptPipe(audio)\n",
        "            transcript = transcript[\"text\"]\n",
        "            inputs = self.tokenizer(transcript, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "            inputs = inputs.to(device)\n",
        "            with torch.no_grad():\n",
        "                text_logits_emo = self.textModel(**inputs).logits\n",
        "                text_probs_emo_raw = F.softmax(text_emo_logits, dim=1)[0]\n",
        "                # Re-order the probabilities to match the output of the audio model\n",
        "                # Please check if the order is correct!!!\n",
        "                text_emo_probs_reordered = torch.tensor([text_probs_emo_raw[4],\n",
        "                                                         text_probs_emo_raw[0],\n",
        "                                                         text_probs_emo_raw[1],\n",
        "                                                         #Adding the probability of \"Surprise\" to \"Fear\"\n",
        "                                                         text_probs_emo_raw[2] + text_probs_emo_raw[6],\n",
        "                                                         text_probs_emo_raw[5],\n",
        "                                                         text_probs_emo_raw[3]]\n",
        "                                                        )\n",
        "\n",
        "\n",
        "        if self.useAudio and self.useText:\n",
        "            # Average the probabilities from both models\n",
        "            emo_probs = (audio_probs_emo + text_emo_probs_reordered) / 2\n",
        "            strength_probs = audio_probs_strength\n",
        "\n",
        "        elif self.useAudio:\n",
        "            emo_probs = audio_probs_emo\n",
        "            strength_probs = audio_probs_strength\n",
        "        elif self.useText:\n",
        "            emo_probs = text_emo_probs_reordered\n",
        "            strength_probs = None\n",
        "        else:\n",
        "            raise ValueError(\"At least one of useAudio or useText must be True.\")\n",
        "\n",
        "        emoList = [\"Neutral\", \"Anger\", \"Disgust\", \"Fear\", \"Sadness\", \"Happiness\"]\n",
        "        strengthList = [\"Weak\", \"Medium\", \"Strong\"]\n",
        "\n",
        "        # Get the predicted emotion and strength\n",
        "        emo_pred = emoList[torch.argmax(emo_probs).item()]\n",
        "        strength_pred = strengthList[torch.argmax(strength_probs).item()] if strength_probs is not None else None\n",
        "\n",
        "        print(f\"Predicted Emotion: {emo_pred}\")\n",
        "        if strength_pred is not None:\n",
        "            print(f\"Predicted Strength: {strength_pred}\")\n",
        "\n",
        "        #Print out the prediction confidence\n",
        "        for i, prob in enumerate(emo_probs):\n",
        "            print(f\"{emoList[i]}: {prob.item():.4f}\")\n",
        "        if strength_probs is not None:\n",
        "            for i, prob in enumerate(strength_probs):\n",
        "                print(f\"{strengthList[i]}: {prob.item():.4f}\")\n",
        "\n",
        "        # Return the predicted emotion and strength for error analysis\n",
        "        return emo_pred, strength_pred\n",
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "mlVenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}